{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1975f95",
   "metadata": {},
   "source": [
    "In machine learning, classification means categorizing the known classes, For example, categorizing the most profitable and non-interested customers from a dataset for advertising a particular product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cabac",
   "metadata": {},
   "source": [
    "Classification is the task of categorizing the known classes based on their features. In most classification problems, machine learning algorithms will do the job, but while classifying a large dataset of images, you will need to use a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c080181",
   "metadata": {},
   "source": [
    "Now let’s come back to classification with neural networks. In this section, I will take you through the task of image classification with neural network using Python. Here, I will be using the famous MNIST fashion dataset, which contains 70,000 clothing fashion images. Here our task is to train an image classification model with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f7f78",
   "metadata": {},
   "source": [
    "I will start this task by importing the necessary Python libraries and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23098e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 11s 0us/step\n",
      "26435584/26421880 [==============================] - 11s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n",
      "4431872/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fashion = keras.datasets.fashion_mnist\n",
    "(xtrain, ytrain), (xtest, ytest) = fashion.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cd837",
   "metadata": {},
   "source": [
    "Before moving forward, let’s have a quick look at one of the samples of the images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076cf838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22c1bd71070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgIndex = 9\n",
    "image = xtrain[imgIndex]\n",
    "print(\"Image Label :\",ytrain[imgIndex])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff92bb2",
   "metadata": {},
   "source": [
    "Now let’s have a look at the shape of both the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e9a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b85db",
   "metadata": {},
   "source": [
    "## Building a Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffa1a9",
   "metadata": {},
   "source": [
    "Now I will build a neural network architecture with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d853bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60a8b6",
   "metadata": {},
   "source": [
    "Before training our model, I will split the training data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c448d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalid, xtrain = xtrain[:5000]/255.0, xtrain[5000:]/255.0\n",
    "yvalid, ytrain = ytrain[:5000], ytrain[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda4b81",
   "metadata": {},
   "source": [
    "## Training a Classification Model with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183071d6",
   "metadata": {},
   "source": [
    "Now here’s how we can train a neural network for the task of image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6824234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.7151 - accuracy: 0.7614 - val_loss: 0.5358 - val_accuracy: 0.8186\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 0.4910 - accuracy: 0.8281 - val_loss: 0.5059 - val_accuracy: 0.8262\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.4448 - accuracy: 0.8431 - val_loss: 0.4183 - val_accuracy: 0.8568\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 29s 17ms/step - loss: 0.4164 - accuracy: 0.8533 - val_loss: 0.4079 - val_accuracy: 0.8628\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 29s 17ms/step - loss: 0.3961 - accuracy: 0.8601 - val_loss: 0.4207 - val_accuracy: 0.8582\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.3817 - accuracy: 0.8645 - val_loss: 0.3727 - val_accuracy: 0.8692\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.3672 - accuracy: 0.8700 - val_loss: 0.3649 - val_accuracy: 0.8734\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 0.3555 - accuracy: 0.8739 - val_loss: 0.3711 - val_accuracy: 0.8722\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 55s 32ms/step - loss: 0.3442 - accuracy: 0.8791 - val_loss: 0.3481 - val_accuracy: 0.8790\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 48s 28ms/step - loss: 0.3366 - accuracy: 0.8807 - val_loss: 0.3570 - val_accuracy: 0.8728\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 43s 25ms/step - loss: 0.3262 - accuracy: 0.8841 - val_loss: 0.3451 - val_accuracy: 0.8770\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 36s 21ms/step - loss: 0.3183 - accuracy: 0.8863 - val_loss: 0.3452 - val_accuracy: 0.8772\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 41s 24ms/step - loss: 0.3124 - accuracy: 0.8879 - val_loss: 0.3332 - val_accuracy: 0.8816\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 50s 29ms/step - loss: 0.3039 - accuracy: 0.8916 - val_loss: 0.3364 - val_accuracy: 0.8792\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 44s 25ms/step - loss: 0.2972 - accuracy: 0.8937 - val_loss: 0.3185 - val_accuracy: 0.8866\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 40s 23ms/step - loss: 0.2914 - accuracy: 0.8959 - val_loss: 0.3218 - val_accuracy: 0.8874\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 45s 26ms/step - loss: 0.2849 - accuracy: 0.8970 - val_loss: 0.3163 - val_accuracy: 0.8852\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 42s 25ms/step - loss: 0.2802 - accuracy: 0.8997 - val_loss: 0.3086 - val_accuracy: 0.8920\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 46s 27ms/step - loss: 0.2747 - accuracy: 0.9007 - val_loss: 0.3154 - val_accuracy: 0.8886\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 47s 27ms/step - loss: 0.2696 - accuracy: 0.9029 - val_loss: 0.3043 - val_accuracy: 0.8902\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 63s 37ms/step - loss: 0.2649 - accuracy: 0.9046 - val_loss: 0.2962 - val_accuracy: 0.8906\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 47s 27ms/step - loss: 0.2603 - accuracy: 0.9069 - val_loss: 0.2967 - val_accuracy: 0.8930\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 44s 26ms/step - loss: 0.2548 - accuracy: 0.9078 - val_loss: 0.3086 - val_accuracy: 0.8892\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 42s 24ms/step - loss: 0.2515 - accuracy: 0.9103 - val_loss: 0.2912 - val_accuracy: 0.8964\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 42s 24ms/step - loss: 0.2467 - accuracy: 0.9112 - val_loss: 0.3086 - val_accuracy: 0.8886\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 48s 28ms/step - loss: 0.2418 - accuracy: 0.9129 - val_loss: 0.3145 - val_accuracy: 0.8866\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 41s 24ms/step - loss: 0.2381 - accuracy: 0.9145 - val_loss: 0.3087 - val_accuracy: 0.8826\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 45s 26ms/step - loss: 0.2352 - accuracy: 0.9159 - val_loss: 0.2936 - val_accuracy: 0.8940\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 45s 26ms/step - loss: 0.2309 - accuracy: 0.9178 - val_loss: 0.2970 - val_accuracy: 0.8904\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 45s 26ms/step - loss: 0.2273 - accuracy: 0.9183 - val_loss: 0.3244 - val_accuracy: 0.8836\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(xtrain, ytrain, epochs=30, \n",
    "                    validation_data=(xvalid, yvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb3934",
   "metadata": {},
   "source": [
    "Now let’s have a look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90329c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [2.806236e-32 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "new = xtest[:5]\n",
    "predictions = model.predict(new)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e22d4",
   "metadata": {},
   "source": [
    "Here is how we can look at the predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901139d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6]\n"
     ]
    }
   ],
   "source": [
    "classes = np.argmax(predictions, axis=1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa872e3",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc29475",
   "metadata": {},
   "source": [
    "Classification is the task of categorizing the known classes based on their features. In most classification problems, machine learning algorithms will do the job, but while classifying a large dataset of images, you will need to use a neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
