{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa6a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928e18cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.49</td>\n",
       "      <td>29.02</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.46</td>\n",
       "      <td>82.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Super Mario Bros.</td>\n",
       "      <td>NES</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>40.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.85</td>\n",
       "      <td>12.88</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.31</td>\n",
       "      <td>35.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.96</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pokemon Red/Pokemon Blue</td>\n",
       "      <td>GB</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "      <td>10.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      Name Platform    Year         Genre Publisher  \\\n",
       "0     1                Wii Sports      Wii  2006.0        Sports  Nintendo   \n",
       "1     2         Super Mario Bros.      NES  1985.0      Platform  Nintendo   \n",
       "2     3            Mario Kart Wii      Wii  2008.0        Racing  Nintendo   \n",
       "3     4         Wii Sports Resort      Wii  2009.0        Sports  Nintendo   \n",
       "4     5  Pokemon Red/Pokemon Blue       GB  1996.0  Role-Playing  Nintendo   \n",
       "\n",
       "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  \n",
       "0     41.49     29.02      3.77         8.46         82.74  \n",
       "1     29.08      3.58      6.81         0.77         40.24  \n",
       "2     15.85     12.88      3.79         3.31         35.82  \n",
       "3     15.75     11.01      3.28         2.96         33.00  \n",
       "4     11.27      8.89     10.22         1.00         31.37  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data\n",
    "\n",
    "data_df = pd.read_csv(r'C:\\Users\\SHREE\\Downloads\\Python CODES\\Video Games Sales Analysis\\vgsales.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2b706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16598 entries, 0 to 16597\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Rank          16598 non-null  int64  \n",
      " 1   Name          16598 non-null  object \n",
      " 2   Platform      16598 non-null  object \n",
      " 3   Year          16327 non-null  float64\n",
      " 4   Genre         16598 non-null  object \n",
      " 5   Publisher     16540 non-null  object \n",
      " 6   NA_Sales      16598 non-null  float64\n",
      " 7   EU_Sales      16598 non-null  float64\n",
      " 8   JP_Sales      16598 non-null  float64\n",
      " 9   Other_Sales   16598 non-null  float64\n",
      " 10  Global_Sales  16598 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Getting som information about data\n",
    "\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88933620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank              0\n",
       "Name              0\n",
       "Platform          0\n",
       "Year            271\n",
       "Genre             0\n",
       "Publisher        58\n",
       "NA_Sales          0\n",
       "EU_Sales          0\n",
       "JP_Sales          0\n",
       "Other_Sales       0\n",
       "Global_Sales      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4707e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank            0\n",
       "Name            0\n",
       "Platform        0\n",
       "Year            0\n",
       "Genre           0\n",
       "Publisher       0\n",
       "NA_Sales        0\n",
       "EU_Sales        0\n",
       "JP_Sales        0\n",
       "Other_Sales     0\n",
       "Global_Sales    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping missing values\n",
    "\n",
    "data_df = data_df.dropna(subset=['Year','Publisher'],axis=0)\n",
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a706b",
   "metadata": {},
   "source": [
    "Plotting Sales VS Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3104c827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "AnnualSalesMarket = data_df.groupby('Year')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].sum().reset_index()\n",
    "plt.plot(AnnualSalesMarket['Year'], AnnualSalesMarket['NA_Sales'], label='North America Sales')\n",
    "plt.plot(AnnualSalesMarket['Year'], AnnualSalesMarket['EU_Sales'], label='Europe Sales')\n",
    "plt.plot(AnnualSalesMarket['Year'], AnnualSalesMarket['JP_Sales'], label='Japan Sales')\n",
    "plt.plot(AnnualSalesMarket['Year'], AnnualSalesMarket['Other_Sales'], label='Other Sales')\n",
    "plt.ylabel('Sales')\n",
    "plt.xlabel('Years')\n",
    "plt.title('Sales VS Years')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a798c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.49</td>\n",
       "      <td>29.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NES</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.85</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GB</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform         Genre Publisher  NA_Sales  EU_Sales\n",
       "0      Wii        Sports  Nintendo     41.49     29.02\n",
       "1      NES      Platform  Nintendo     29.08      3.58\n",
       "2      Wii        Racing  Nintendo     15.85     12.88\n",
       "3      Wii        Sports  Nintendo     15.75     11.01\n",
       "4       GB  Role-Playing  Nintendo     11.27      8.89"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unwanted columns\n",
    "\n",
    "data = data_df.drop(['Rank','Name','Year','JP_Sales','Other_Sales','Global_Sales'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95948da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>289</td>\n",
       "      <td>41.49</td>\n",
       "      <td>29.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>289</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>289</td>\n",
       "      <td>15.85</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>289</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>289</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform  Genre  Publisher  NA_Sales  EU_Sales\n",
       "0        19      2        289     41.49     29.02\n",
       "1         3      6        289     29.08      3.58\n",
       "2        19      8        289     15.85     12.88\n",
       "3        19      2        289     15.75     11.01\n",
       "4        26      9        289     11.27      8.89"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the object data type columns to string\n",
    "# Using Label Encoder for coversion of categorical columns to numerical columns \n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Platform'] = le.fit_transform(data['Platform'].astype('str'))\n",
    "data['Genre'] = le.fit_transform(data['Genre'].astype('str'))\n",
    "data['Publisher'] = le.fit_transform(data['Publisher'].astype('str'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12df371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_df['Global_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a3dd4",
   "metadata": {},
   "source": [
    "Feature Scaling\n",
    "\n",
    "Why is scaling important in machine learning?\n",
    "Scaling the target value is a good idea in regression modelling; scaling of the data makes it easy for a model to learn and understand the problem. Scaling of the data comes under the set of steps of data pre-processing when we are performing machine learning algorithms in the data set.\n",
    "\n",
    "Here we are scaling the data by standardizing it using Standard Scaler.\n",
    "\n",
    "- Standardizing the data\n",
    "\n",
    "Data Standardization - \n",
    "\n",
    "Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1. The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values\n",
    "\n",
    "Standard Scaler - \n",
    "\n",
    "StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way. StandardScaler can be influenced by outliers (if they exist in the dataset) since it involves the estimation of the empirical mean and standard deviation of each feature.\n",
    "\n",
    "In Machine Learning, StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8435208",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9262a",
   "metadata": {},
   "source": [
    "Splitting the data into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1365f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13134745",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "- When can I use linear regression?\n",
    "\n",
    "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used.\n",
    "\n",
    "You can use simple linear regression when you want to know: How strong the relationship is between two variables (e.g. the relationship between rainfall and soil erosion). The value of the dependent variable at a certain value of the independent variable (e.g. the amount of soil erosion at a certain level of rainfall).\n",
    "\n",
    "Linear Regression, intuitively is a regression algorithm with a Linear approach. We try to predict a continuous value of a given data point by generalising on the data that we have in hand. The linear part indicates that we are using a linear approach in generalising over the data.\n",
    "\n",
    "Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.\n",
    "\n",
    "Linear Regression, as the name suggests, simply means fitting a line to the data that establishes a relationship between a target 'y' variable with the explanatory 'x' variables.\n",
    "\n",
    "- How is linear regression used in real life?\n",
    "\n",
    "Medical researchers often use linear regression to understand the relationship between drug dosage and blood pressure of patients. For example, researchers might administer various dosages of a certain drug to patients and observe how their blood pressure responds.\n",
    "\n",
    "- How do you choose the best linear regression model?\n",
    "\n",
    "When choosing a linear model, these are factors to keep in mind:\n",
    "\n",
    "Only compare linear models for the same dataset.\n",
    "\n",
    "Find a model with a high adjusted R2.\n",
    "\n",
    "Make sure this model has equally distributed residuals around zero.\n",
    "\n",
    "Make sure the errors of this model are within a small bandwidth.\n",
    "\n",
    "- How do you know if a linear relationship is statistically significant?\n",
    "\n",
    "To determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as α or alpha) of 0.05 works well. An α of 0.05 indicates that the risk of concluding that a correlation exists—when, actually, no correlation exists—is 5%.\n",
    "\n",
    "- In a regression context, the slope is the heart and soul of the equation because it tells you how much you can expect Y to change as X increases. In general, the units for slope are the units of the Y variable per units of the X variable.\n",
    "\n",
    "- How do you know if a linear regression is appropriate?\n",
    "\n",
    "If a linear model is appropriate, the histogram should look approximately normal and the scatterplot of residuals should show random scatter . If we see a curved relationship in the residual plot, the linear model is not appropriate. Another type of residual plot shows the residuals versus the explanatory variable.\n",
    "\n",
    "- There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other.\n",
    "\n",
    "- The correlation\n",
    "\n",
    "R: The correlation between the predictor variable, x, and the response variable, y. \n",
    "\n",
    "R2: The proportion of the variance in the response variable that can be explained by the predictor variable in the regression model.\n",
    "\n",
    "- What does the regression coefficient tell us?\n",
    "\n",
    "In regression with a single independent variable, the coefficient tells you how much the dependent variable is expected to increase (if the coefficient is positive) or decrease (if the coefficient is negative) when that independent variable increases by one.\n",
    "\n",
    "- The Correlation Coefficient\n",
    "\n",
    "When the r value is closer to +1 or -1, it indicates that there is a stronger linear relationship between the two variables. A correlation of -0.97 is a strong negative correlation while a correlation of 0.10 would be a weak positive correlation.\n",
    "\n",
    "- How do you know if a regression coefficient is significant?\n",
    "\n",
    "If the p-value is less than the chosen threshold then it is significant. The significance of a regression coefficient in a regression model is determined by dividing the estimated coefficient over the standard deviation of this estimate.\n",
    "\n",
    "- What is R value in regression?\n",
    "\n",
    "Simply put, R is the correlation between the predicted values and the observed values of Y. R square is the square of this coefficient and indicates the percentage of variation explained by your regression line out of the total variation. This value tends to increase as you include additional predictors in the model.\n",
    "\n",
    "- What does a positive R value mean?\n",
    "\n",
    "The correlation coefficient r ranges between -1 and +1. A positive r values indicates that as one variable increases so does the other, and an r of +1 indicates that knowing the value of one variable allows perfect prediction of the other.\n",
    "\n",
    "- What is a good regression coefficient?\n",
    "\n",
    "A value of 1.0 indicates a perfect fit, and is thus a highly reliable model for future forecasts, while a value of 0.0 would indicate that the calculation fails to accurately model the data at all.\n",
    "\n",
    "- How is r2 value calculated?\n",
    "\n",
    "R 2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 . The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared.\n",
    "\n",
    "- How do you increase R-squared in regression?\n",
    "\n",
    "When more variables are added, r-squared values typically increase. They can never decrease when adding a variable; and if the fit is not 100% perfect, then adding a variable that represents random data will increase the r-squared value with probability 1.\n",
    "\n",
    "- How do you interpret R & r2?\n",
    "\n",
    "The coefficient of determination, R2, is similar to the correlation coefficient, R. The correlation coefficient formula will tell you how strong of a linear relationship there is between two variables. R Squared is the square of the correlation coefficient, r (hence the term r squared).\n",
    "\n",
    "The most common interpretation of r-squared is how well the regression model fits the observed data. For example, an r-squared of 60% reveals that 60% of the data fit the regression model. Generally, a higher r-squared indicates a better fit for the model.\n",
    "\n",
    "- Should I report R or R-squared?\n",
    "\n",
    "If strength and direction of a linear relationship should be presented, then r is the correct statistic. If the proportion of explained variance should be presented, then r² is the correct statistic.\n",
    "\n",
    "- Is higher adjusted R-squared better?\n",
    "\n",
    "A higher R-squared value indicates a higher amount of variability being explained by our model and vice-versa. If we had a really low RSS value, it would mean that the regression line was very close to the actual points. This means the independent variables explain the majority of variation in the target variable\n",
    "\n",
    "- What does P value mean in linear regression?\n",
    "\n",
    "P-Value is a statistical test that determines the probability of extreme results of the statistical hypothesis test,taking the Null Hypothesis to be correct. It is mostly used as an alternative to rejection points that provides the smallest level of significance at which the Null-Hypothesis would be rejected.\n",
    "\n",
    "- When interpreting a correlation coefficient it is important to look at?\n",
    "\n",
    "The correct answer is a) Scores on one variable plotted against scores on a second variable. 3. When interpreting a correlation coefficient, it is important to look at: The +/– sign of the correlation coefficient.\n",
    "\n",
    "- How do you interpret correlation and regression results?\n",
    "\n",
    "Both quantify the direction and strength of the relationship between two numeric variables. When the correlation (r) is negative, the regression slope (b) will be negative. When the correlation is positive, the regression slope will be positive.\n",
    "\n",
    "- How do you tell if a regression model is a good fit?\n",
    "\n",
    "If the model fit to the data were correct, the residuals would approximate the random errors that make the relationship between the explanatory variables and the response variable a statistical relationship. Therefore, if the residuals appear to behave randomly, it suggests that the model fits the data well.\n",
    "\n",
    "- How do you interpret a linear regression model?\n",
    "\n",
    "Linear Regression is the most talked-about term for those who are working on ML and statistical analysis. Linear Regression, as the name suggests, simply means fitting a line to the data that establishes a relationship between a target 'y' variable with the explanatory 'x' variables.\n",
    "\n",
    "- What are the advantages and disadvantages of linear regression?\n",
    "\n",
    "Advantages\n",
    "\n",
    "Linear regression performs exceptionally well for linearly separable data\t\n",
    "\n",
    "Easier to implement, interpret and efficient to train\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "The assumption of linearity between dependent and independent variables\n",
    "\n",
    "It is often quite prone to noise and overfitting\n",
    "\n",
    "- What are the advantages of linear regression?\n",
    "\n",
    "The principal advantage of linear regression is its simplicity, interpretability, scientific acceptance, and widespread availability. Linear regression is the first method to use for many problems. Analysts can use linear regression together with techniques such as variable recoding, transformation, or segmentation.\n",
    "\n",
    "- What are the limitations of linear regression?\n",
    "\n",
    "Main limitation of Linear Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. It assumes that there is a straight-line relationship between the dependent and independent variables which is incorrect many times.\n",
    "\n",
    "- What is Karl Pearson coefficient of correlation?\n",
    "\n",
    "Karl Pearson's coefficient of correlation is defined as a linear correlation coefficient that falls in the value range of -1 to +1. Value of -1 signifies strong negative correlation while +1 indicates strong positive correlation.\n",
    "\n",
    "- What are the three strengths of linear regression?\n",
    "\n",
    "Three major uses for regression analysis are (1) determining the strength of predictors, (2) forecasting an effect, and (3) trend forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39efdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model creating & fitting the data in the model\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16af4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90796563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 95.97%\n",
      "Test Accuracy : 97.23%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "Acc_reg = reg.score(X_train, y_train)\n",
    "acc_reg = reg.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_reg*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_reg*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b54caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.09\n",
      "Variance Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Checking error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, y_pred))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd24581",
   "metadata": {},
   "source": [
    "### Support Vector Regressor\n",
    "\n",
    "An intuitive overview of Support vector Regression -\n",
    "\n",
    "Support Vector Machines(SVM) are one of the state-of-the-art machine learning algorithm based on Maximal Margin Classifier.\n",
    "SVM support linear as well as non-linear regression called Support Vector Regression(SVR).\n",
    "\n",
    "Support vector machines (SVM) are a set of supervised learning methods; used for regression, classification. Unlike other learning methods; SVM tries to fit the best decision boundary or hyperplane using some data samples from given training data which are called support vectors. Hyperplane means a flat affine subspace of the dimension p-1 in p dimensional space; in machine learning lingo p are the features.\n",
    "\n",
    "Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points.\n",
    "\n",
    "The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "\n",
    "- What is the objective of the support vector machine algorithm?\n",
    "\n",
    "The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points. The dimension of the hyperplane depends upon the number of features. If the number of input features is two, then the hyperplane is just a line.\n",
    "\n",
    "The goal of SVM is to divide the datasets into classes to find a maximum marginal hyperplane (MMH).\n",
    "\n",
    "To find the optimal hyperplane with maximum margin; some of the training data points which helps to find it are called support vectors. Support vectors determine the shape of the hyperplane. Other than support vectors does not play any role to decide optimal decision hyperplane.That’s it this is the intuitive explanation of SVM.\n",
    "\n",
    "Let me give you an analogy:\n",
    "\n",
    "Linear Regression : Support vector regression : : Logistic regression : Support vector Machine\n",
    "\n",
    "The difference dwells on the input feature space dimensions.\n",
    "A linear regression models a line to classify the data points, while a support vector regression can also model a hyperplane.\n",
    "\n",
    "- Hyperplane\n",
    "\n",
    "Hyperplane acts as a classifier that splits the input feature space.\n",
    "\n",
    "A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1.\n",
    "\n",
    "For a two-variable space, the hyperplane would be a line b0+b1*x1+b2*x2\n",
    "\n",
    "Where the coefficients (B1 and B2) that determine the slope of the line and the intercept (B0) are found by the learning algorithm, and X1 and X2 are the two input variables.\n",
    "\n",
    "You can make classifications using this line. By plugging in input values into the line equation, you can calculate whether a new point is above or below the line.\n",
    "\n",
    "Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).\n",
    "Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).\n",
    "A value close to the line returns a value close to zero and the point may be difficult to classify.\n",
    "If the magnitude of the value is large, the model may have more confidence in the prediction.\n",
    "\n",
    "In case of SVM,\n",
    "\n",
    "A point above the line gets a label `1`\n",
    "\n",
    "A point below the line gets a label `0`\n",
    "\n",
    "A point close to the line return a value close to `0` and the point may be difficult to classify.\n",
    "\n",
    "If the magnitude of value is large for a test data point, the model will have more confidence in prediction.\n",
    "Thus, comes the concept of maximising the margin.\n",
    "\n",
    "- Margin\n",
    "\n",
    "The points closest to the hyperplane has an uncertain class label or has a class label with nearly equal probability as the other class label.\n",
    "\n",
    "Margin is the perpendicular distance between the hyperplane and the closest points.\n",
    "\n",
    "Therefore, SVM tries to maximise this margin and works as `Maximal Margin Classifier`\n",
    "\n",
    "The distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane.\n",
    "\n",
    "The margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane.\n",
    "\n",
    "The hyperplane is learned from training data using an optimization procedure that maximizes the margin.\n",
    "\n",
    "- Support Vectors\n",
    "\n",
    "The training instances closest to the hyperplane that help define the margin are called Support Vectors.\n",
    "\n",
    "Real data is messy!\n",
    "\n",
    "Thus, we can’t have a perfect hyperplane splitting the data points perfectly into two classes.\n",
    "\n",
    "We tend to relax the constraint of maximising the margin and allow some points to violate the margin(that is, allow some points to have wrong label as per our model for a good overall performance).\n",
    "\n",
    "- SVR working\n",
    "\n",
    "SVR tries to have as many support vectors as possible within the boundary lines without much margin violation, thus keeping the error within the threshold decided by the boundary lines.\n",
    "\n",
    "Intuitively,these support vectors contribute to the error made by the SVR and thus, we want most of the support vectors to be in that threshold.\n",
    "\n",
    "We can model SVM as well as SVR through kernels that indicates the similarity measure between the test data point and the support vectors.\n",
    "\n",
    "- Support Vector Machines (Kernels)\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel.\n",
    "\n",
    "The learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra, which is out of the scope of this introduction to SVM.\n",
    "\n",
    "A powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "For example, the inner product of the vectors [2, 3] and [5, 6] is 2*5 + 3*6 or 28.\n",
    "\n",
    "The equation for making a prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:\n",
    "\n",
    "f(x) = B0 + sum(ai * (x,xi))\n",
    "\n",
    "This is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data. The coefficients B0 and ai (for each input) must be estimated from the training data by the learning algorithm.\n",
    "\n",
    "- Linear Kernel SVM\n",
    "\n",
    "The dot-product is called the kernel and can be re-written as:\n",
    "\n",
    "K(x, xi) = sum(x * xi)\n",
    "\n",
    "The kernel defines the similarity or a distance measure between new data and the support vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because the distance is a linear combination of the inputs.\n",
    "\n",
    "Other kernels can be used that transform the input space into higher dimensions such as a Polynomial Kernel and a Radial Kernel. This is called the Kernel Trick.\n",
    "\n",
    "It is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers.\n",
    "\n",
    "- Polynomial Kernel SVM\n",
    "\n",
    "Instead of the dot-product, we can use a polynomial kernel, for example:\n",
    "\n",
    "K(x,xi) = 1 + sum(x * xi)^d\n",
    "\n",
    "Where the degree of the polynomial must be specified by hand to the learning algorithm. When d=1 this is the same as the linear kernel. The polynomial kernel allows for curved lines in the input space.\n",
    "\n",
    "- Radial Kernel SVM\n",
    "\n",
    "Finally, we can also have a more complex radial kernel. For example:\n",
    "\n",
    "K(x,xi) = exp(-gamma * sum((x – xi^2))\n",
    "\n",
    "Where gamma is a parameter that must be specified to the learning algorithm. A good default value for gamma is 0.1, where gamma is often 0 < gamma < 1. The radial kernel is very local and can create complex regions within the feature space, like closed polygons in two-dimensional space.\n",
    "\n",
    "- How to Learn a SVM Model\n",
    "\n",
    "The SVM model needs to be solved using an optimization procedure.\n",
    "\n",
    "You can use a numerical optimization procedure to search for the coefficients of the hyperplane. This is inefficient and is not the approach used in widely used SVM implementations like LIBSVM. If implementing the algorithm as an exercise, you could use stochastic gradient descent.\n",
    "\n",
    "There are specialized optimization procedures that re-formulate the optimization problem to be a Quadratic Programming problem. The most popular method for fitting SVM is the Sequential Minimal Optimization (SMO) method that is very efficient. It breaks the problem down into sub-problems that can be solved analytically (by calculating) rather than numerically (by searching or optimizing).\n",
    "\n",
    "- Data Preparation for SVM\n",
    "\n",
    "This section lists some suggestions for how to best prepare your training data when learning an SVM model.\n",
    "\n",
    "- Numerical Inputs: \n",
    "\n",
    "SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).\n",
    "\n",
    "- Binary Classification: \n",
    "\n",
    "Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.\n",
    "\n",
    "- Conclusion\n",
    "\n",
    "Lastly, a major difference between Linear regression and SVR lies on the fact that Linear regression tends to minimize the error and SVR tends to keep it within a threshold.\n",
    "\n",
    "You can, easily implement SVR(for regression tasks) and SVM(for classification purposes) using the model provided by the skicit-learn library.\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "regressor=SVR(kernel=’linear’,degree=1)\n",
    "\n",
    "Then, fit the model and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019033cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the model & fitting data\n",
    "\n",
    "model_svr = SVR()\n",
    "model_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f630489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on test data\n",
    "\n",
    "pred_svr = model_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef95bc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 65.55%\n",
      "Test Accuracy : 45.08%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "Acc_svr = model_svr.score(X_train, y_train)\n",
    "acc_svr = model_svr.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_svr*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_svr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b63b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.80\n",
      "Variance Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Checking error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, pred_svr))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, pred_svr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ac83d",
   "metadata": {},
   "source": [
    "### K Neighbors Regressor\n",
    "\n",
    "KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood.\n",
    "\n",
    "The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f9e69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model & fitting the training data to model\n",
    "\n",
    "neigh = KNeighborsRegressor()\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d121a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "\n",
    "pred_neigh = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa4ac973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 96.44%\n",
      "Test Accuracy : 81.63%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "\n",
    "Acc_neigh = neigh.score(X_train, y_train)\n",
    "acc_neigh = neigh.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_neigh*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_neigh*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25ef1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.60\n",
      "Variance Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Checking the error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, pred_neigh))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, pred_neigh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff212f31",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor\n",
    "\n",
    "- Ensembles and boosting\n",
    "\n",
    "Machine learning models can be fitted to data individually, or combined in an ensemble. An ensemble is a combination of simple individual models that together create a more powerful new model.\n",
    "\n",
    "Machine learning boosting is a method for creating an ensemble. It starts by fitting an initial model (e.g. a tree or linear regression) to the data. Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly. The combination of these two models is expected to be better than either model alone. Then you repeat this process of boosting many times.  Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models.\n",
    "\n",
    "- Gradient boosting explained\n",
    "\n",
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error. \n",
    "\n",
    "- How are the targets calculated? \n",
    "\n",
    "The target outcome for each case in the data depends on how much changing that case's prediction impacts the overall prediction error.\n",
    "\n",
    "- Prediction error\n",
    "\n",
    "If a small change in the prediction for a case causes a large drop in error, then next target outcome of the case is a high value. Predictions from the new model that are close to its targets will reduce the error.\n",
    "If a small change in the prediction for a case causes no change in error, then next target outcome of the case is zero. Changing this prediction does not decrease the error.\n",
    "The name gradient boosting arises because target outcomes for each case are set based on the gradient of the error with respect to the prediction. Each new model takes a step in the direction that minimizes prediction error, in the space of possible predictions for each training case.\n",
    "\n",
    "- Why is boosting used?\n",
    "\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor.\n",
    "\n",
    "Gradient Boosting Machine (GBM) A Gradient Boosting Machine or GBM combines the predictions from multiple decision trees to generate the final predictions. Keep in mind that all the weak learners in a gradient boosting machine are decision trees.\n",
    "\n",
    "In gradient boosting, we predict and adjust our predictions in the opposite (negative gradient) direction. This achieves the opposite (minimize the loss). Since, the loss of a model inversely relates to its performance and accuracy, doing so improves its performance.\n",
    "\n",
    "Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem.\n",
    "\n",
    "- How does gradient boosting Regressor work?\n",
    "\n",
    "Gradient boosting algorithm is one of the most powerful algorithms in the field of machine learning. As we know that the errors in machine learning algorithms are broadly classified into two categories i.e. Bias Error and Variance Error. As gradient boosting is one of the boosting algorithms it is used to minimize bias error of the model.\n",
    "\n",
    "Unlike, Adaboosting algorithm, the base estimator in the gradient boosting algorithm cannot be mentioned by us. The base estimator for the Gradient Boost algorithm is fixed and i.e. Decision Stump. Like, AdaBoost, we can tune the n_estimator of the gradient boosting algorithm. However, if we do not mention the value of n_estimator, the default value of n_estimator for this algorithm is 100.\n",
    "\n",
    "Gradient boosting algorithm can be used for predicting not only continuous target variable (as a Regressor) but also categorical target variable (as a Classifier). When it is used as a regressor, the cost function is Mean Square Error (MSE) and when it is used as a classifier then the cost function is Log loss.\n",
    "\n",
    "Gradient boosting involves three elements:\n",
    "\n",
    "A loss function to be optimized.\n",
    "\n",
    "A weak learner to make predictions.\n",
    "\n",
    "An additive model to add weak learners to minimize the loss function.\n",
    "\n",
    "1. Loss Function\n",
    "\n",
    "The loss function used depends on the type of problem being solved.\n",
    "\n",
    "It must be differentiable, but many standard loss functions are supported and you can define your own.\n",
    "\n",
    "For example, regression may use a squared error and classification may use logarithmic loss.\n",
    "\n",
    "A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used.\n",
    "\n",
    "2. Weak Learner\n",
    "\n",
    "Decision trees are used as the weak learner in gradient boosting.\n",
    "\n",
    "Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions.\n",
    "\n",
    "Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.\n",
    "\n",
    "Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels.\n",
    "\n",
    "It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.\n",
    "\n",
    "This is to ensure that the learners remain weak, but can still be constructed in a greedy manner.\n",
    "\n",
    "3. Additive Model\n",
    "\n",
    "Trees are added one at a time, and existing trees in the model are not changed.\n",
    "\n",
    "A gradient descent procedure is used to minimize the loss when adding trees.\n",
    "\n",
    "Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.\n",
    "\n",
    "Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.\n",
    "\n",
    "Generally this approach is called functional gradient descent or gradient descent with functions.\n",
    "\n",
    "One way to produce a weighted combination of classifiers which optimizes [the cost] is by gradient descent in function space\n",
    "\n",
    "— Boosting Algorithms as Gradient Descent in Function Space\n",
    "\n",
    "The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model.\n",
    "\n",
    "A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.\n",
    "\n",
    "Improvements to Basic Gradient Boosting\n",
    "\n",
    "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.\n",
    "\n",
    "It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.\n",
    "\n",
    "In this this section we will look at 4 enhancements to basic gradient boosting:\n",
    "\n",
    "Tree Constraints\n",
    "\n",
    "Shrinkage\n",
    "\n",
    "Random sampling\n",
    "\n",
    "Penalized Learning\n",
    "\n",
    "1. Tree Constraints\n",
    "\n",
    "It is important that the weak learners have skill but remain weak.\n",
    "\n",
    "There are a number of ways that the trees can be constrained.\n",
    "\n",
    "A good general heuristic is that the more constrained tree creation is, the more trees you will need in the model, and the reverse, where less constrained individual trees, the fewer trees that will be required.\n",
    "\n",
    "Below are some constraints that can be imposed on the construction of decision trees:\n",
    "\n",
    "Number of trees, generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed.\n",
    "Tree depth, deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels.\n",
    "Number of nodes or number of leaves, like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used.\n",
    "Number of observations per split imposes a minimum constraint on the amount of training data at a training node before a split can be considered\n",
    "Minimim improvement to loss is a constraint on the improvement of any split added to a tree.\n",
    "\n",
    "2. Weighted Updates\n",
    "\n",
    "The predictions of each tree are added together sequentially.\n",
    "\n",
    "The contribution of each tree to this sum can be weighted to slow down the learning by the algorithm. This weighting is called a shrinkage or a learning rate.\n",
    "\n",
    "Each update is simply scaled by the value of the “learning rate parameter v”\n",
    "\n",
    "The effect is that learning is slowed down, in turn require more trees to be added to the model, in turn taking longer to train, providing a configuration trade-off between the number of trees and learning rate.\n",
    "\n",
    "Decreasing the value of v [the learning rate] increases the best value for M [the number of trees].\n",
    "\n",
    "— Greedy Function Approximation: A Gradient Boosting Machine\n",
    "\n",
    "It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1.\n",
    "\n",
    "Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.\n",
    "\n",
    "- Stochastic Gradient Boosting\n",
    "\n",
    "3. Stochastic Gradient Boosting\n",
    "\n",
    "A big insight into bagging ensembles and random forest was allowing trees to be greedily created from subsamples of the training dataset.\n",
    "\n",
    "This same benefit can be used to reduce the correlation between the trees in the sequence in gradient boosting models.\n",
    "\n",
    "This variation of boosting is called stochastic gradient boosting.\n",
    "\n",
    "at each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to fit the base learner.\n",
    "\n",
    "A few variants of stochastic boosting that can be used:\n",
    "\n",
    "Subsample rows before creating each tree.\n",
    "\n",
    "Subsample columns before creating each tree\n",
    "\n",
    "Subsample columns before considering each split.\n",
    "\n",
    "Generally, aggressive sub-sampling such as selecting only 50% of the data has shown to be beneficial.\n",
    "\n",
    "According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling\n",
    "\n",
    "4. Penalized Gradient Boosting\n",
    "\n",
    "Additional constraints can be imposed on the parameterized trees in addition to their structure.\n",
    "\n",
    "Classical decision trees like CART are not used as weak learners, instead a modified form called a regression tree is used that has numeric values in the leaf nodes (also called terminal nodes). The values in the leaves of the trees can be called weights in some literature.\n",
    "\n",
    "As such, the leaf weight values of the trees can be regularized using popular regularization functions, such as:\n",
    "\n",
    "L1 regularization of weights.\n",
    "L2 regularization of weights.\n",
    "The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions.\n",
    "\n",
    "- How does gradient boosting predict?\n",
    "\n",
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.\n",
    "\n",
    "It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.\n",
    "\n",
    "Applications: Gradient Boosting Algorithm is generally used when we want to decrease the Bias error.\n",
    "\n",
    "- What is a gradient boosted decision tree?\n",
    "\n",
    "Gradient-boosted decision trees are a machine learning technique for optimizing the predictive value of a model through successive steps in the learning process.\n",
    "\n",
    "- 1st-Estimator\n",
    "\n",
    "For estimator-1, the root level (level 0) will consist of all the records. The outcome at this level is equal to the mean of the entire predictor column (addition of all values in Predictor column divided by a number of records). Let us find out what is the MSE for this level. MSE is calculated as the mean of the square of errors. Here error is equal to actual predictor-predicted outcome. The outcome for the particular node is always equal to the mean of prdictor records of that node. So, the MSE of the root node of the 1st estimator is calculated as given below.\n",
    "\n",
    "                                 MSE=(∑(Agei –mu)2 )/9=577.11\n",
    "\n",
    "The cost function hers is MSE and the objective of the algorithm here is to minimize the MSE.\n",
    "\n",
    "- Decision Stump\n",
    "\n",
    "Now, one of the independent variables will be used by the Gradient boosting to create the Decision Stump. Let us suppose that the x1 is used here for prediction. So, the records with False x1 will go in one child node, and records with True x2 will go in another child node.\n",
    "\n",
    "- 2nd-Estimator:\n",
    "\n",
    "Let us now find out the estimator-2. Unlike AdaBoost, in the Gradient boosting algorithm, residuals of the first estimator are taken as root nodes. Let us suppose for this estimator another dependent variable is used for prediction. So, the records with False x2 will go in one child node, and records with True x2 will go in another child node.\n",
    "\n",
    "The prediction of predictor here is slightly tricky. First, the predictor will be predicted from estimator 1 as per the value of x1, and then the mean from the estimator is found out with the help of the value of x2 and then that means is added to x predicted from the first estimator and that is the final prediction of Gradient boosting with two estimators.\n",
    "\n",
    "The final MSE is much better than the MSE of the root node of the 1st Estimator. This is only for 2 estimators. There can be n number of estimators in gradient boosting algorithm.\n",
    "\n",
    "The most important thing in this algorithm is to find the best value of n_estimators. Can Find the best estimator with GridSearchCV.\n",
    "\n",
    "- General Approach for Parameter Tuning\n",
    "\n",
    "Choose a relatively high learning rate.\n",
    "\n",
    "Determine the optimum number of trees for this learning rate.\n",
    "\n",
    "Tune tree-specific parameters for decided learning rate and number of trees.\n",
    "\n",
    "Lower the learning rate and increase the estimators proportionally to get more robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bf68dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model & fitting the training data\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438936de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "\n",
    "pred_gbr = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7be0716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 98.18%\n",
      "Test Accuracy : 84.71%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "Acc_gbr = gbr.score(X_train, y_train)\n",
    "acc_gbr = gbr.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_gbr*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_gbr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05b77a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.50\n",
      "Variance Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Checking error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, pred_gbr))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, pred_gbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d4ce6",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "\n",
    "- What is decision tree intuition?\n",
    "\n",
    "Decision Trees are easy & Simple to implement & interpreted. Decision Tree is a diagram (flow) that is used to predict the course of action or a probability. Each branch of the decision tree represents an outcome or decision or a reaction.\n",
    "\n",
    "Decision Tree Learning is a classic algorithm used in machine learning for classification and regression purposes. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. Each path from the root to a leaf of the tree signifies a decision process.\n",
    "\n",
    "Conceptually, decision trees are quite simple. We split a dataset into smaller and smaller groups, attempting to make each one as “pure” or “homogenous” as possible. Once we finish splitting, we use the final groups to make predictions on unseen data.\n",
    "\n",
    "- What is main goal of decision tree learning?\n",
    "\n",
    "The goal of this algorithm is to create a model that predicts the value of a target variable, for which the decision tree uses the tree representation to solve the problem in which the leaf node corresponds to a class label and attributes are represented on the internal node of the tree.\n",
    "\n",
    "Decision trees use a criteria (there are multiple criteria available) to decide to split a node in two or more sub-nodes\n",
    "The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable\n",
    "Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "- What is a child node decision tree?\n",
    "\n",
    "In a decision tree building process, two important decisions are to be made — what is the best split(s) and which is the best variable to split a node. Information Gain criteria helps in making these decisions. Using a independent variable value(s), the child nodes are created.\n",
    "\n",
    "- Important Terminology related to Decision Trees\n",
    "\n",
    "Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "\n",
    "Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "\n",
    "Leaf / Terminal Node: Nodes do not split is called Leaf or Terminal node.\n",
    "\n",
    "Pruning: When we remove sub-nodes of a decision node, this process is called pruning.\n",
    "\n",
    "Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n",
    "\n",
    "Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "- What constitutes a good split?\n",
    "\n",
    "A successful decision tree is one that does a good job of “splitting” data into homogeneous groups. Therefore, in order to build a good decision tree algorithm, we’ll need a method for evaluating splits.\n",
    "One of the most popular methods for assessing the quality of a split is called “Gini Impurity”.\n",
    "\n",
    "Interpreting Gini Impurity is simple. A value of 0 means that a group is perfectly homogenous. In other words, all records in the group belong to the same class. A value of 0.5, on the other hand, means that a group is completely split between classes.\n",
    "When we evaluate potential splits, we want to choose the one that has the lowest Gini Impurity (ie. the one that is closest to perfect “purity”).\n",
    "\n",
    "- Calculating Gini Impurity is a bit convoluted, but it helps cement the intuition behind decision tree splits.\n",
    "\n",
    "First, we calculate the percentage of records that fall into each class (p). Then we square those numbers and add them together. Finally, we subtract that number from one.\n",
    "Because decision trees split data into more than one group, our final step is to calculate the weighted average of the Gini Impurity in each group.\n",
    "Note the difference between a “group” (also known as a “node” in decision tree terminology) and a “class”. Groups or nodes are what we create when we do a split. Classes are our target variables — the thing we are trying to predict.\n",
    "\n",
    "To start, we’ll need to choose a column and value to split on. Ex. Let’s use Sales Prior Year ≥ $100.\n",
    "Next, we separate our dataset into two groups — those with prior sales ≥ $100 and those with prior sales <$100.\n",
    "Third, we apply the Gini Impurity formula to each node.\n",
    "\n",
    "Finally, we take the weighted average of the two numbers. Because each node has the same number of records (n) this amounts to a simple average:\n",
    "\n",
    "Note that our first node (where prior sales < $100) has a Gini Impurity of 0 because all of the records fall into the same class. Our second node, on the other hand, has a Gini Impurity of .44 because class membership is divided.\n",
    "Now that we know how to evaluate the quality of each split.\n",
    "\n",
    "- How do we decide what to split on?\n",
    "\n",
    "In our above example, we evaluated the Gini Impurity on one potential split. But how do we know this was the right choice? And how do we know what to split on next?\n",
    "The simple answer is that we can’t know unless we try all options. So that’s exactly what we do.\n",
    "At any given node, we loop through all of the unique values in all of our columns and choose the one with the lowest Gini Index. By definition, this is what makes the decision tree a greedy algorithm — one in which we select the most optimal choice at each step.\n",
    "\n",
    "Once we identify the lowest Gini Impurity among these options, we make the split and then run the same process on each of the resulting nodes.\n",
    "\n",
    "- When do we stop splitting?\n",
    "\n",
    "We’re nearly there. The last step is to figure out when to stop splitting.\n",
    "But first, why do we need to stop at all? To answer that question, think about what would happen if we put no restrictions on splitting and we had a complex dataset…\n",
    "Chances are that our algorithm would be able to find lots of weird combinations of features to reach homogeneity for every node. This, of course, would be textbook overfitting. In other words, these decision rules would probably perform terribly on unseen data because they are far too complex.\n",
    "In order to solve this problem (at least partially), we set up rules for when we want to stop a split. There are many rules we can draw from; below are the two most common:\n",
    "Max depth: Here we set a limit on the depth of our tree. Depth is simply a measure of how far we are from the start of our tree. For example, if we set a max depth of two, we would stop expanding after splitting twice:\n",
    "\n",
    "- Minimum samples split: \n",
    "\n",
    "This is the minimum number of records that must be in a node in order to split. If we set our minimum samples to 50, then we would stop splitting any node with less than 50 records.\n",
    "\n",
    "- Closing Thoughts\n",
    "\n",
    "When we put it all together, decision trees are pretty easy to grasp. We take a dataset and split it continuously. Each time we split, we look at all possible options and choose the one that creates the most pure groups. Finally, we add some rules that govern when and how we stop splitting.\n",
    "The wonderful thing about tree-based models is that they build upon each other neatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "096ce5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model & fitting training data\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0140621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test data\n",
    "\n",
    "pred_dtr = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec77231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 99.78%\n",
      "Test Accuracy : 83.97%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "Acc_dtr = dtr.score(X_train, y_train)\n",
    "acc_dtr = dtr.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_dtr*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_dtr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2710333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.53\n",
      "Variance Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Checking error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, pred_dtr))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, pred_dtr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e70ba4",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "The random forest is an algorithm that aggregates numerous, independently trained decision trees to make predictions. This “wisdom of the crowd” approach allows it to learn from a diverse set of data, leading to more stable and generalizable results.\n",
    "\n",
    "- How does a random forest Regressor work?\n",
    "\n",
    "Random forest is a type of supervised learning algorithm that uses ensemble methods (bagging) to solve both regression and classification problems. The algorithm operates by constructing a multitude of decision trees at training time and outputting the mean/mode of prediction of the individual trees.\n",
    "\n",
    "Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.\n",
    "\n",
    "The fundamental idea behind a random forest is to combine many decision trees into a single model. Individually, predictions made by decision trees (or humans) may not be accurate, but combined together, the predictions will be closer to the mark on average.\n",
    "\n",
    "1. Bagging – \n",
    "\n",
    "Suppose we have a dataset, and we make different models on the same dataset and combine it, will it be useful? No right? There is a high chance we’ll get the same results since we are giving the same input. So instead we use a technique called bootstrapping. In this, we create subsets of the original dataset with replacement. The size of the subsets is the same as the size of the original set. Since we do this with replacement so there is a high chance that we provide different data points to our models.\n",
    "\n",
    "2. Boosting – \n",
    "\n",
    "Suppose any data point in your observation has been incorrectly classified by your 1st model, and then the next (probably all the models), will combine the predictions provide better results? Off-course it’s a big NO.\n",
    "\n",
    "Boosting technique is a sequential process, where each model tries to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n",
    "\n",
    "It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example, ADA BOOST, XG BOOST.\n",
    "\n",
    "- What is the Random Forest Algorithm?\n",
    "\n",
    "Random Forest is a technique that uses ensemble learning, that combines many weak classifiers to provide solutions to complex problems.\n",
    "\n",
    "As the name suggests random forest consists of many decision trees. Rather than depending on one tree it takes the prediction from each tree and based on the majority votes of predictions, predicts the final output.\n",
    "\n",
    "It consists of 3 components which are the root node, decision node, leaf node. The node from where the population starts dividing is called a root node. The nodes we get after splitting a root node are called decision nodes and the node where further splitting is not possible is called a leaf node. The question comes how do we know which feature will be the root node? In a dataset there can be 100’s of features so how do we decide which feature will be our root node. To answer this question, we need to understand something called the “Gini Index”\n",
    "\n",
    "- What is Gini Index?\n",
    "\n",
    "To select a feature to split further we need to know how impure or pure that split will be. A pure sub-split means that either you should be getting “yes” or “no”.\n",
    "\n",
    "gini index for Random Forest Algorithm\n",
    "\n",
    "We will see what output we get after splitting, taking each feature as our root node.\n",
    "\n",
    "splitting for Random Forest Algorithm\n",
    "\n",
    "When we take feature 1 as our root node, we get a pure split whereas when we take feature 2, the split is not pure. So how do we know that how much impurity this particular node has? This can be understood with the help of the “Gini Index”.\n",
    "\n",
    "We basically need to know the impurity of our dataset and we’ll take that feature as the root node which gives the lowest impurity or say which has the lowest Gini index.\n",
    "\n",
    "gini formula\n",
    "\n",
    "Where P+ is the probability of a positive class and P_ is the probability of a negative class.\n",
    "\n",
    "- Steps involved in Random Forest Algorithm\n",
    "\n",
    "Step-1 – We first make subsets of our original data. We will do row sampling and feature sampling that means we’ll select rows and columns with replacement and create subsets of the training dataset\n",
    "\n",
    "Step- 2 – We create an individual decision tree for each subset we take\n",
    "\n",
    "Step-3 – Each decision tree will give an output\n",
    "\n",
    "Step 4 – Final output is considered based on Majority Voting if it’s a classification problem and average if it’s a regression problem.\n",
    "\n",
    "- How do you increase the accuracy of a random forest Regressor?\n",
    "\n",
    "If you want to increase the accuracy of your model, increase the number of trees. Specify the maximum number of features to be included at each node split. This depends very heavily on your dataset. If your independent variables are highly correlated, you'll want to decrease the maximum number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53a8709f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the model & fitting the train data\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b28e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on test data\n",
    "\n",
    "pred_rfr = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4b595db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 99.13%\n",
      "Test Accuracy : 83.89%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "\n",
    "Acc_rfr = rfr.score(X_train, y_train)\n",
    "acc_rfr = rfr.score(X_test, y_test)\n",
    "print ('Train Accuracy : {:.2f}%'.format(Acc_rfr*100))\n",
    "print ('Test Accuracy : {:.2f}%'.format(acc_rfr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96d73794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.53\n",
      "Variance Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Checking error & r2 score\n",
    "\n",
    "print('Mean squared error: %.2f'% mean_squared_error(y_test, pred_rfr))\n",
    "print('Variance Score: %.2f'% r2_score(y_test, pred_rfr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81162a8",
   "metadata": {},
   "source": [
    "### Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "284fd2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.972324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>0.450810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Neighbors Regressor</td>\n",
       "      <td>0.816287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Regressor</td>\n",
       "      <td>0.847138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>0.839698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>0.838905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy\n",
       "0            Linear Regression  0.972324\n",
       "1     Support Vector Regressor  0.450810\n",
       "2        K Neighbors Regressor  0.816287\n",
       "3  Gradient Boosting Regressor  0.847138\n",
       "4      Decision Tree Regressor  0.839698\n",
       "5      Random Forest Regressor  0.838905"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({\"Model\":['Linear Regression','Support Vector Regressor','K Neighbors Regressor',\n",
    "                                'Gradient Boosting Regressor','Decision Tree Regressor',\n",
    "                               'Random Forest Regressor'],\n",
    "                      \"Accuracy\":[acc_reg, acc_svr, acc_neigh, acc_gbr, acc_dtr, acc_rfr]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fb625be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Accuracy', ylabel='Model'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAEGCAYAAAAXPP1HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArS0lEQVR4nO3dd9weVZ3+8c9F6CSGFhSREEBIEAiBhADSy6ILKiC4gEEFXFlUmi4qimLEFUGsgAgsPw0gIIsUKUonoZNGGlUFUZSVKr0lXL8/5jzLzeNT7iRPn+v9ej2vZ+4zM+d856TMd86Ze0a2iYiIiHpaorcDiIiIiN6TRCAiIqLGkghERETUWBKBiIiIGksiEBERUWNL9nYAEc1addVVPWLEiN4OIyKiX5kxY8ZTtoe1tz6JQPQbI0aMYPr06b0dRkREvyLp0Y7WZ2ogIiKixpIIRERE1FimBqLfuP+xpxn7pXN7O4yIiB414+RPdmv9GRGIiIiosSQCERERNZZEICIiosaSCERERNRYEoGIiIgaSyIQERFRY0kEIiIiaiyJQERERI0lEYiIiKixJAIRERE1lkRgIUl6sY2yQyV17zMg/7nNyZIelDRb0jRJY3qy/Y5I+oikY3o7joiI6FzeNdAFbJ/RnfVLEiDbb7ZaNcH2dEkHAScD/9IFbQ2yvWBx6rB9BXDF4sYSERHdLyMCXUDSRElHl+XJkk6SNFXSQ5K2LeWDJJ1crt7nSPqPUj5Y0o2SZkqaK2mPUj5C0v2STgdmAmt2EMKdwBplvxUk/by0c09DfctL+p/S9kWS7pY0rqx7UdLxku4GtpJ0QIl/lqQzS+yDJE2SNK/E+YWy7xGS7iv1/qqUHSjptLK8Vjm+OeX38FI+SdIpku6Q9LCkfbr4jyUiIpqQEYHusaTt8ZJ2A74J7AJ8GnjO9uaSlgFul3Qd8BdgL9vPS1oVuEtSy9X0SOAg25/rpL0PApeX5WOBm2wfLGlFYKqkG4DPAs/aHi1pI2BWw/4rAPNsHydpA+ArwNa23yiJyATgXmAN2xsBlLoBjgHWtv1aQ1mj04BzbZ8j6WDgFGDPsm51YBtgFNUIwq9b7yzpEOAQgKWHrNJJN0RExMJKItA9Li2/ZwAjyvKuwOiGK9+hwHrAY8AJkrYD3qS6sn9n2eZR23d10M75klYABgGbNbTzkZYRCmBZYDjVCfcnALbnSZrTUM8C4JKyvDMwFphWzUiwHPAEcCWwjqRTgauB68r2c0ocl/NWMtJoK+CjZfk84HsN6y4v0x33SXrnP+1ZxXoWcBbACu9a2+11RERELJokAt3jtfJ7AW/1sYDDbV/buKGkA4FhwNhyBf4nqpM3wEudtDMBmA2cCPyU6oQrYG/bD7ZqRx3U82rDfQECzrH91dYbSdoE+ADweeDfgIOB3YHtgI8A35C0YScxN57MX2tY7ii+iIjoJrlHoOdcC3xW0lIAktYvV/NDgSdKErAjsNbCVGr7DeDrwJZlWP9a4PCWE7+kTcumt1GdvJH0PmDjdqq8EdhH0mpl25XLPP+qwBK2LwG+AWwmaQlgTds3A18GVgQGt6rvDmC/sjyhxBEREX1ERgQW3vKSHmv4/MMm9zubappgZjlJP0k1V34+cKWk6VTz9g8sbEC2X5H0A+Bo4DDgx8Cc0s6fgA8BpwPnlCmBe6iG9J9ro677JH0duK6c6N+gGgF4BfhFKQP4KtWUxC8lDaW6ov+R7X+0Gnw4Avi5pC+VYz5oYY8vIiK6j+xMu9aBpEHAUrZflbQu1ZX/+rZf7+XQmrbCu9b2qE98q7fDiIjoUTNOXrzH1EiaYXtce+szIlAfywM3l6kJAZ/tT0lARER0jyQCNWH7BaDdjDAiIuopNwtGRETUWBKBiIiIGksiEBERUWNJBCIiImosiUBERESNJRGIiIiosSQCERERNZbnCES/scF7VmH6Yj5hKyIi3i4jAhERETWWRCAiIqLGkghERETUWBKBiIiIGksiEBERUWNJBCIiImosXx+MfuP1x+/lz8dv3NthxAAx/Li5vR1CRJ+QEYGIiIgaSyIQERFRY0kEIiIiaiyJQERERI0lEYiIiKixJAIRERE1lkQgIiKixpIIRERE1FgSgYiIiBpLIhAREVFjSQQiIiJqrE8nApKOlXSvpDmSZknaohdjOUrS8m2UT5T03VZlYyTdv5D1ryjpc10Q558kzS19NkXSWotbZ0REDFx9NhGQtBXwIWAz26OBXYC/9FIsg4CjgH9KBIALgX1ble0HXLCQzawILFQiUOJqy46lzyYDX1/IONpqR5J65O+KpLwIKyKiB/XZRABYHXjK9msAtp+y/Tf4v6veVcvyOEmTy/JESedJuknS7yV9ppTvIOkWSZdJuk/SGS0nNkn7lyvoeZJOamlc0ouSjpd0N3As8G7gZkk3NwZp+0HgH61GK/4N+JWkdSVdI2mGpFsljSp1v7PEMrv8vB84EVi3jHycXE6+J5e45krat+FYbpZ0AdDZ69PuBNYo+w2TdImkaeVn64by6yXNlHSmpEclrSpphKT7JZ0OzATWlPSlsu8cSd8q+68g6epyHPMa4jyx9PUcSd8vZWtJurGU3ShpeCmfJOmHpW9P+ufDiIiI7tKXr76uA46T9BBwA3CR7SlN7Dca2BJYAbhH0tWlfDzwPuBR4Brgo5LuoDrxjAWeBa6TtKfty8v+82wfByDpYKor7afaaPNCqlGAuyVtCTxt+/eSbgQOLctbAKcDOwGnAFNs71Wu6gcDxwAb2R5T2tsbGANsAqwKTJN0S8OxbGT7kU764oPA5WX5J8CPbN9WTsDXAhsA3wRusv1dSR8EDmnYfyRwkO3PSdoVWK+0LeAKSdsBw4C/2d69xD1U0srAXsAo25a0YqnvNOBc2+eU/jwF2LOsWx/YxfaCxgOQdEhLTGsMXaqTw42IiIXVZ0cEbL9IdYI+BHgSuEjSgU3s+hvbr5QT9s1UJy6AqbYfLieaC4FtgM2BybaftD0fOB/Yrmy/ALikyXB/BexTRhn2Ay6UNBh4P3CxpFnAmVSjHFAlAz8rx7nA9nNt1LkNcGFZ/3dgSom35Vg6SgJulvQE1XRKyxTFLsBpJZYrgHdIGlLa+VWJ5RqqhKjFo7bvKsu7lp97qEYIRlElBnOBXSSdJGnbcizPA68CZ0v6KPByqWOrhnjOK223uLh1ElBiOsv2ONvjVl6hvZmQiIhYVH15RIByYpgMTJY0F/gUMAmYz1tJzLKtd2vnc1vl6qD5V9s6MbUT518k/QnYHtib6oS3BPCPliv8RdBRbC91su+OZZtJwPHAF0s8W9l+5W2NSM22I+C7ts/8p0ClscBuwHclXWf7eEnjgZ2pEqPDqJKf1hr/TDo7poiI6AZ9dkRA0khJ6zUUjaEa1gf4E9VoAVQn3kZ7SFpW0irADsC0Uj5e0trlqn1f4DbgbmD7Mic+CNif6sq7LS8AQzoI+ULgR8AfbT9m+3ngEUkfK8cjSZuUbW8EPlvKB0l6Rxv13wLsW9YPoxqpmNpB+29TTvhHAZ8sQ/XXUZ2QKe2OKYu3Ud3TQBn+X6mdKq8FDi4jHUhaQ9Jqkt4NvGz7l8D3gc3KNkNt/7bE0NLWHVSJAcCE0nZERPSiPpsIUM2bn9NywxnV/P7Esu5bwE8k3Uo1hN9oKnA1cBfw7ZYbDKlunDsRmAc8Alxm+3Hgq1RTCLOBmbZ/0048ZwG/U6ubBRtcDGxIGWYvJgCfljQbuBfYo5QfCexYRjlmABvafhq4vdxwdzJwGTCnxHUT8GXb/9tO220qx3ch8HngCGBcuVHvPuDQstm3gF0lzQT+FXicKilpXdd1VMP6d5a4f02VuGwMTC1TDscC/1XKryp/blOAL5RqjgAOKuWfKP0QERG9SHbrEfP+S9JE4EXb329VvgNwtO0P9UJYfZqkZYAFtuer+srmzxZjOqNbjV5jOV/1H+/t7TBigBh+XGdfuokYGCTNsD2uvfV9+h6B6BHDgf8pUyavA5/p5XgiIqIHDahEwPbEdsonU910GK3Y/j2waW/HERERvaMv3yMQERER3SyJQERERI0lEYiIiKixJAIRERE1lkQgIiKixpIIRERE1NiA+vpgDGxLr74hw4+b3tthREQMKBkRiIiIqLEkAhERETWWRCAiIqLGkghERETUWBKBiIiIGksiEBERUWP5+mD0Gw888QBbn7p1b4cREf3A7Yff3tsh9BsZEYiIiKixJAIRERE1lkQgIiKixpIIRERE1FgSgYiIiBpLIhAREVFjSQQiIiJqLIlAREREjSURiIiIqLEkAhERETWWRCAiIqLGBlwiIOnFhuXdJP1e0vBW2xwo6U1JoxvK5kka0UndZ0t6XyfbTJK0TxvlO0i6qukDWQTluJ6UNEvSA5K+0J3tRURE/zfgEoEWknYGTgU+aPvPbWzyGHDswtRp+99t39cV8S0sSYOa3PQi22OArYFjJa3ZBW33yMupVBmwfycjIvqiAfmfrqRtgf8Gdrf9x3Y2uwrYUNLINvbfVdKdkmZKuljS4FI+WdK4svxpSQ+Vsv+WdFpDFdtJukPSw61GB94h6TJJ90k6o+WkJ2l/SXPLqMRJDXG8KOl4SXcDW0k6sew7R9L3O+oD208DfwBWL3UdIGlqGS04syWxaO84ysjGDyXdDJwkaV1J10iaIelWSaPKdh8rcc+WdEsp27ChrTmS1ivlXyzbzpN0VCkbIel+SacDM4HFTlwiIqJ5AzERWAb4DbCn7Qc62O5N4HvA1xoLJa0KfB3YxfZmwHTgi622eTfwDWBL4F+AUa3qXh3YBvgQcGJD+XjgP4GNgXWBj5a6TgJ2AsYAm0vas2y/AjDP9hbAfcBewIa2RwP/1VEnlOmQZYE5kjYA9gW2LqMFC4AJTRzH+qUf/hM4Czjc9ljgaOD0ss1xwAdsbwJ8pJQdCvyktDUOeEzSWOAgYIvS3mckbVq2Hwmca3tT24+2Oo5DJE2XNP2NF9/o6JAjImIRDMRE4A3gDuDTTWx7AbClpLUbyrYE3gfcLmkW8ClgrVb7jQem2H7G9hvAxa3WX277zTKN8M6G8qm2H7a9ALiQKlnYHJhs+0nb84Hzge3K9guAS8ry88CrwNmSPgq83M4x7SvpXuBhqpPxq8DOwFhgWjmmnYF1mjiOi20vKCMi7wcuLvufSRlpAG4HJkn6DNAyfXEn8DVJXwHWsv1KOdbLbL9k+0XgUmDbsv2jtu9q62Bsn2V7nO1xSw1eqp1DjoiIRTUQE4E3gX+jurL+WkcblhPvD4CvNBQLuN72mPLzPtutkwp1EsNr7Wzr1iF0UterJWloiXU8VWKwJ3BNO/tcZHtDqpPsDyS9q7RxTsMxjbQ9sYnjeKn8XgL4R8P+Y2xvUOI6lGoEZU1glqRVbF9ANTrwCnCtpJ06aeulDtZFREQ3GoiJALZfphqWnyCps5GBScAuwLDy+S5ga0nvBZC0vKT1W+0zFdhe0krlRrq9mwxtvKS1y70B+wK3AXeXulYt8/b7A1Na71iuyofa/i1wFNU0Qrts3wmcBxwJ3AjsI2m1UtfKktZq9jhsPw88IuljZX9J2qQsr2v7btvHAU8Ba0paB3jY9inAFcBo4BZgz9KfK1BNc9zaXLdFRER3GZCJAIDtZ4APAl+XtEcH270OnAKsVj4/CRwIXChpDlViMKrVPn8FTqA6id9ANX//XBNh3Ul1z8A84BGqofLHga8CNwOzgZm2f9PGvkOAq0pMU4Bmvhp4EtW8/F+ortqvK/tfD6y+kMcxAfi0pNnAvUBLn57ccqMj1cl+NlWSM69MI4yimv+fSZV0TS3tnW37niaOISIiupHs1qPV0QxJg22/WK6kLwN+bvuy3o5rYfWn4xg8fLA3+dImvR1GRPQDtx9+e2+H0GdImmF7XHvrO/x+uKSVO1pfrrrraqKkXajuzL8OuLx3w1lkA+U4IiJiEXT2oJgZtH9Dm6nuPK8l20f3dgxdYaAcR0RELJoOEwHba3e0PiIiIvq3pm4WLHeJHyDpG+XzcEnjuze0iIiI6G7NfmvgdGAr4OPl8wvAT7slooiIiOgxzb5MZgvbm0m6B8D2s5KW7sa4IiIiogc0OyLwRnnYjQEkDaN6gl9ERET0Y80mAqdQfcd8NUnfoXoi3gndFlVERET0iKYfKFReO7sz1VcJb7R9f3cGFtHauHHjPH369N4OIyKiX+nKBwo9QfXGvP9bV/MHCkVERPR7C/NAoeHAs2V5ReDPQJ4zEBER0Y91eI+A7bVtrwNcC3zY9qq2V6F6s9+lPRFgREREdJ9mbxbcvLz+FgDbvwO2756QIiIioqc0+xyBpyR9Hfgl1VTBAcDT3RZVRERE9IhmRwT2B4ZRfYXwcmC1UhYRERH9WFMjAuXbAUdKegfwpu0XuzesiIiI6AlNJQKSNgbOBVYun58CPmV7XjfGFvE2Lzz4IFO2y60pEdF1tr9lSm+H0OuanRo4E/ii7bVsrwX8J3BW94UVERERPaHZRGAF2ze3fLA9GVihWyKKiIiIHtPstwYelvQN4Lzy+QDgke4JKSIiInpKsyMCB1N9a+BSqm8ODAMO6q6gIiIiomc0+62BZ4EjujmWiIiI6GGdvXToio7W2/5I14YTERERPamzEYGtgL9QvXXwbqoXDkVERMQA0Vki8C7gX6ieIvhx4GrgQtv3dndgERER0f06e/vgAtvX2P4UsCXwB2CypMN7JLqIiIjoVp3eLChpGWB3qlGBEcAp5BXEERERA0KHIwKSzgHuADYDvmV7c9vftv3XziqW9E5JF0h6WNIMSXdK2mtxgpU0UdLRZfl4SbssYj1jJO3WzrodJD0naZakOZJukLTa4sTdqv4Rkj7e8HmcpFO6qO6Jkv5aYr9PUl4MFRERHersOQKfANYHjgTukPR8+XlB0vPt7SRJVG8pvMX2OrbHAvsB72lj22YfavQ2to+zfcOi7AuMAdpMBIpbbY+xPRqYBnx+Edtpywiq+y0AsD3ddld+NfNHtscAewBnSlpqcStc1D+jRWhnUE+0ExERb+nsHoElbA8pP+9o+Bli+x0d7LoT8LrtMxrqetT2qQCSDpR0saQrgeskDZZ0o6SZkuZK2qNlP0nHSnpQ0g3AyIbySZL2KctjJU0pIw/XSlq9lE+WdJKkqZIekrStpKWB44F9y5Xzvu0dRElohgDPls8rS7q8jBTcJWl0J+XblzZmSbpH0hDgRGDbUvaFMgJxVdl+oqSfl7gflnREQyzfkPSApOslXdgyMtLBn93vgZeBlcr+X5I0rcT4rc7qLTGcIGkK1Zsn2+vjI8rowxxJv2rvuFU5WdK88me8b9l2B0k3S7oAmNvRMUVERNfrriu9DYGZnWyzFTDa9jPlinMv289LWhW4S9UzDDajGknYtMQ6E5jRWEm54j0V2MP2k+UE8x2qpyECLGl7vKqpgG/a3kXSccA424e1E9u2kmYBqwAvAV8r5d8C7rG9p6SdqN7IOKaD8qOBz9u+XdJg4FXgGOBo2x8q8e/Qqu1RwI5UCciDkn4GbALs3VE/tCZpM+D3tp+QtCuwHjCe6iugV0jajipR6KjeFW1vX/p4Sjt9fAywtu3XJK1Y9mvruD9a+mQTYFVgmqRbyvbjgY1s/9NjqyUdAhwC8M5llunokCMiYhH01JDvT4FtqEYJNi/F19t+pmUT4IRycnoTWAN4J7AtcJntl0s9bT3gaCSwEXB9dQHPIODxhvUtNzbOoBqWb8atDSfqrwDfAw4tx7A3gO2bJK0iaWgH5bcDP5R0PnCp7cdKjB252vZrwGuSnij9sA3wG9uvlJiu7GD/L0j6DLAO8MFStmv5uad8HkyVGAzppN6Lyu+O+ngOcL6ky6mmg2jnuLeh+urpAuDvZaRhc+B5YGpbSQCA7bMob7ocOWSIOzjuiIhYBM2+a2Bh3Ut1NQ+A7c8DO1O9o6DFSw3LE8q6sWV+++/Asi27d9KWgHvLnP4Y2xvb3rVh/Wvl9wIWLfG5Atiuoa3W3F657ROBfweWoxrlGNVEe681LLfEvDAPcvqR7ZHAvsC5kpYt+3+3oY/ea/v/NVFvy59RR328O/BTYCwwQ9KS7Rx3R2291MG6iIjoRt2VCNwELCvpsw1ly3ew/VDgCdtvSNoRWKuU3wLsJWm5Mr/+4Tb2fRAYJmkrqKYKJG3YSXwvUF0NN2Mb4I8N8Uwo7ewAPGX7+fbKJa1re67tk4DpVMP+C9N2i9uAD0tatgy1797ZDrYvLW1+CrgWOLjsi6Q1VH0Totl62+xjSUsAa5ZXVH8ZWBEY3M5x30J1X8YgScOokqupC9kPERHRxbplasC2Je0J/EjSl4Enqa76vtLOLucDV0qaDswCHij1zJR0USl7FLi1jbZeV3XT4CllOH5J4MdUoxLtuRk4ptwH8F3bF7Va33KPgIDnqK5uASYCv5A0h2p+/VOdlB9VEpsFwH3A76imPuZLmg1M4q3h+nbZnlamRWaXfphe4urM8cAFwAbl584ytP8icECz9XbQxw8BvyxlohqN+Iekb7dx3K9T3Rcym2oU5cu2/7fJUZKIiOgmsjPt2h9IGmz7RUnLU11dH2K7sxsye63e7jByyBCftelmnW8YEdGk7W+Z0tshdDtJM2yPa299j9wsGF3iLEnvo7p34pwuPFl3V70REdEPJBHoJ2x/vPOt+k69ERHRP3TXzYIRERHRDyQRiIiIqLEkAhERETWWRCAiIqLGkghERETUWBKBiIiIGksiEBERUWN5jkD0G0NGjqzFU8AiInpSRgQiIiJqLIlAREREjSURiIiIqLEkAhERETWWRCAiIqLGkghERETUWL4+GP3GE489x2n/eWVvhxERA8BhP/hwb4fQZ2REICIiosaSCERERNRYEoGIiIgaSyIQERFRY0kEIiIiaiyJQERERI0lEYiIiKixJAIRERE1lkQgIiKixpIIRERE1Fi/TQQkLZA0S9K9kmZL+qKkRToeScdL2qWD9YdK+uSiRwuSNi7xzpL0jKRHyvINi1NvqzYOlPRkqfcBSV/oqrojImJg6s/vGnjF9hgASasBFwBDgW8ubEW2j+tk/RmLEmCrOuYCYwAkTQKusv3rxm0kLWl7/mI2dZHtwyStAjwo6de2/7I4FXZRXM20I0C23+zutiIiotJvRwQa2X4COAQ4TJVBkk6WNE3SHEn/0bKtpC9LmltGEU4sZZMk7VOWT5R0X9nv+6VsoqSjy/IYSXeV9ZdJWqmUT5Z0kqSpkh6StG0zsZf9TpA0BThS0lhJUyTNkHStpNXLdutKuqaU3yppVCd98jTwB6Bl/wNKbLMknSlpUCn/dIl3sqT/lnRaQ5/8UNLNwEnttS/pY5Lmlf68pZRt2NDWHEnrlfIvlm3nSTqqlI2QdL+k04GZwJrN9FtERHSN/jwi8Da2Hy5TA6sBewDP2d5c0jLA7ZKuA0YBewJb2H5Z0sqNdZTPewGjbFvSim00dS5wuO0pko6nGoE4qqxb0vZ4SbuV8nanG1pZ0fb2kpYCpgB72H5S0r7Ad4CDgbOAQ23/XtIWwOnATu1VKGk4sCwwR9IGwL7A1rbfKCfdCWVa4hvAZsALwE3A7IZq1gd2sb1A0o3ttH8c8AHbf23or0OBn9g+X9LSwCBJY4GDgC0AAXeX5OdZYCRwkO3PtXEch1Aleaw0ZFiT3RkREc0aMIlAofJ7V2B0y1U+1ZTBelQn5l/YfhnA9jOt9n8eeBU4W9LVwFVvq1waSnXSnlKKzgEubtjk0vJ7BjBiIeK+qPweCWwEXF+NkjMIeFzSYOD9wMWlHGCZduraV9KOpa7P2H5V0s7AWGBa2X854AlgPDClpR8kXUx18m9xcUkCOmr/dmCSpP9pOP47gWMlvQe4tCQP2wCX2X6ptHUpsC1wBfCo7bvaOhjbZ1ElQQx/13pu55gjImIRDZhEQNI6wAKqE5yortqvbbXNB4F2Tya250saD+wM7AccRgdX3W14rfxewML17UstIQL32t6qcaWkdwD/aLknohMt9whsBVwt6Xel3nNsf7VVvXs1GdcS7bVv+9AyQrA7MEvSGNsXSLq7lF0r6d95K0nrqJ2IiOhhA+IeAUnDgDOA02wbuBb4bBlqR9L6klYArgMOlrR8KW89NTAYGGr7t1TD/WMa19t+Dni2Yf7/E1RD+V3lQWBYOYkjaSlJG9p+HnhE0sdKuSRt0lFFtu8EzgOOBG4E9lF1UyWSVpa0FjAV2F7SSpKWBPZup65225e0ru27yw2XTwFrlqTsYdunUF3xjwZuAfaUtHz5s9gLuHWReyoiIrpEfx4RWE7SLGApYD7VSe+HZd3ZVEPzM1WNZT8J7Gn7GkljgOmSXgd+C3ytoc4hwG8kLUt1BdvW1+8+BZxRkomHqea9u4Tt18t0xillGmJJ4MfAvcAE4GeSvl6O+Ve8fT6/LSdR3YB3AvB14LpyH8UbwOdt3yXpBOBu4G/AfcBz7dTVXvsnl5sBRZVwzAaOAQ6Q9Abwv8Dxtp9R9W2JqaW+s23fI2lEs/0TERFdT9UFdNSVpMG2XywjApcBP7d9WW/H1Zbh71rPX57ww843jIjoxGE/+HBvh9BjJM2wPa699QNiaiAWy8QysjIPeAS4vFejiYiIHtWfpwaiC9g+urdjiIiI3pMRgYiIiBpLIhAREVFjSQQiIiJqLIlAREREjSURiIiIqLEkAhERETWWRCAiIqLG8hyB6DdWe8/QWj0NLCKiJ2REICIiosaSCERERNRYEoGIiIgaSyIQERFRY0kEIiIiaiyJQERERI3l64PRbzz+yB/5zgH79HYYEdHPHfvLX/d2CH1KRgQiIiJqLIlAREREjSURiIiIqLEkAhERETWWRCAiIqLGkghERETUWBKBiIiIGksiEBERUWNJBCIiImosiUBERESNJRGIiIiosT6bCEhaIGmWpHmSrpS0YhfVe6Ck07qirlb1Tpb0YIl5lqRueSi+pBGSPt7BuldK+/dJOlfSUt0RR0REDAx9NhEAXrE9xvZGwDPA53s7oCZMKDGPsd3UWy0kLeyLn0YAbSYCxR9tjwE2Bt4D/NtC1v9PFiHGxWlrUE+1FRERfTsRaHQnsAaApPGS7pB0T/k9spQfKOlSSddI+r2k77XsLOkgSQ9JmgJs3VC+lqQbJc0pv4eX8kmSfibpZkkPS9pe0s8l3S9pUrNBS1pZ0uWl/rskjS7lEyWdJek64FxJwyRdImla+dm6bLd9wwjDPZKGACcC25ayL7TXtu0FwNSGfhsraYqkGZKulbR6Kd+8xHenpJMlzWvoz4slXQlcJ2mF0gfTSix7lO02lDS1xDNH0npl26slzS4jOvuWbXcu+84tdS1Tyv8k6ThJtwEfa7Z/IyJi8fX51xCXK8Sdgf9Xih4AtrM9X9IuwAnA3mXdGGBT4DXgQUmnAvOBbwFjgeeAm4F7yvanAefaPkfSwcApwJ5l3UrATsBHgCupEoh/B6ZJGmN7Vhvhni/plbK8MzARuMf2npJ2As4tMVLi2cb2K5IuAH5k+7aSjFwLbAAcDXze9u2SBgOvAscAR9v+UCf9tiywBXBkmR44FdjD9pPlxPwd4GDgF8Ahtu+QdGKrarYCRtt+RtIJwE22Dy7TNFMl3QAcCvzE9vmSlgYGAbsBf7O9e4llaIlnErCz7YcknQt8FvhxaetV29u0cRyHAIcADF1+uY4OOSIiFkFfHhFYTtIs4GlgZeD6Uj4UuLhcuf4I2LBhnxttP2f7VeA+YC2qk+Fk20/afh24qGH7rYALyvJ5QOOJ6ErbBuYCf7c91/abwL1Uw/NtaZwaeLrUdx6A7ZuAVSQNLdteYbsladgFOK0c7xXAO8rV/+3ADyUdAaxoe34nfQawbkO//dn2HGAksBFwfVn3deA95YQ+xPYdZd8LWtV1ve1nyvKuwDFl/8nAssBwqtGar0n6CrBWOaa5wC6STpK0re3nSgyP2H6o1HcOsF1DW41/Lv/H9lm2x9ket8KyyzRx+BERsTD6ciLwSpnrXgtYmrfuEfg2cHO5d+DDVCekFq81LC/grREPN9lm43Ytdb3Zqt43aX4kRR208VJD2RLAVg1JxBq2X7B9ItUoxHLAXZJGNdFmyz0C7wW2lPSREse9DfVvbHvXduJr1BijgL0b6hhu+37bF1CNmrwCXCtpp3KyH0uVEHxX0nEL2VZERPSQvpwIAFCuJo8Aji5D3EOBv5bVBzZRxd3ADpJWKfs3zkHfAexXlicAt3VJ0G+5pdSLpB2Ap2w/38Z21wGHtXyQNKb8XreMRJwETAdGAS8AQzpr2PbjVNMIXwUeBIZJ2qrUu5SkDW0/C7wgacuy235t1wZU0xWHS1KpY9Pyex3gYdunUI1mjJb0buBl278Evg9sRjWlM0LSe0t9nwCmdHYcERHRvfp8IgBg+x5gNtWJ6ntUV5m3U81Hd7bv41Rz9XcCNwAzG1YfARwkaQ7VienIro2cicC4Uv+JwKfa2e6Ilu0k3Uc17w5wVLnZbjbVFffvgDnA/HIjXrs3CxaXA8tTTY/sA5xU6poFvL9s82ngLEl3Ul21P9dOXd8GlgLmlGmZb5fyfYF5ZcpgFNV9EBtT3UMwCzgW+K8yXXMQ1bTOXKqRlTM6iT8iIrqZqmnwqCtJg22/WJaPAVa33dUJUZdYY5WV/Ll/3bm3w4iIfu7YXzb17e4BQ9IM2+PaW9/nvzUQ3W53SV+l+rvwKM1Nt0RExACRRKDmbF9EO3fsR0TEwNcv7hGIiIiI7pFEICIiosaSCERERNRYEoGIiIgaSyIQERFRY0kEIiIiaixfH4x+Y/W1163dg0AiIrpbRgQiIiJqLIlAREREjSURiIiIqLG8dCj6DUkvUL1Sue5WBZ7q7SB6Wfqgkn6opB867oO1bA9rb8fcLBj9yYMdvUGrLiRNr3s/pA8q6YdK+mHx+iBTAxERETWWRCAiIqLGkghEf3JWbwfQR6Qf0gct0g+V9MNi9EFuFoyIiKixjAhERETUWBKBiIiIGksiEH2OpA9KelDSHyQd08Z6STqlrJ8jabPeiLM7NdEHE8qxz5F0h6RNeiPO7tZZPzRst7mkBZL26cn4ekoz/SBpB0mzJN0raUpPx9jdmvg3MVTSlZJmlz44qDfi7E6Sfi7pCUnz2lm/aP832s5PfvrMDzAI+COwDrA0MBt4X6ttdgN+BwjYEri7t+PuhT54P7BSWf7XgdYHzfZDw3Y3Ab8F9untuHvp78OKwH3A8PJ5td6Ouxf64GvASWV5GPAMsHRvx97F/bAdsBkwr531i/R/Y0YEoq8ZD/zB9sO2Xwd+BezRaps9gHNduQtYUdLqPR1oN+q0D2zfYfvZ8vEu4D09HGNPaObvAsDhwCXAEz0ZXA9qph8+Dlxq+88AtgdaXzTTBwaGSBIwmCoRmN+zYXYv27dQHVd7Fun/xiQC0desAfyl4fNjpWxht+nPFvb4Pk11FTDQdNoPktYA9gLO6MG4elozfx/WB1aSNFnSDEmf7LHoekYzfXAasAHwN2AucKTtN3smvD5jkf5vzCOGo69RG2Wtv+PazDb9WdPHJ2lHqkRgm26NqHc00w8/Br5ie0F1ITggNdMPSwJjgZ2B5YA7Jd1l+6HuDq6HNNMHHwBmATsB6wLXS7rV9vPdHFtfskj/NyYRiL7mMWDNhs/vocrwF3ab/qyp45M0Gjgb+FfbT/dQbD2pmX4YB/yqJAGrArtJmm/78h6JsGc0+2/iKdsvAS9JugXYBBgoiUAzfXAQcKKryfI/SHoEGAVM7ZkQ+4RF+r8xUwPR10wD1pO0tqSlgf2AK1ptcwXwyXKH7JbAc7Yf7+lAu1GnfSBpOHAp8IkBdNXXWqf9YHtt2yNsjwB+DXxugCUB0Ny/id8A20paUtLywBbA/T0cZ3dqpg/+TDUigqR3AiOBh3s0yt63SP83ZkQg+hTb8yUdBlxLdafwz23fK+nQsv4MqrvDdwP+ALxMdSUwYDTZB8cBqwCnl6vh+R5gb19rsh8GvGb6wfb9kq4B5gBvAmfbbvMrZv1Rk38Xvg1MkjSXaoj8K7YH1KuJJV0I7ACsKukx4JvAUrB4/zfmEcMRERE1lqmBiIiIGksiEBERUWNJBCIiImosiUBERESNJRGIiIiosSQCEVE7kvaSZEmjejuWiN6WRCAi6mh/4DaqB9N0C0mDuqvuiK6URCAiakXSYGBrqnc07FfKBkn6vqS55T3uh5fyzSXdUd5xP1XSEEkHSjqtob6rJO1Qll+UdLyku4GtJB0naZqkeZLOKm/GQ9J7Jd1Q6p0paV1J50nao6He8yV9pKf6JeoriUBE1M2ewDXl0czPSNoMOARYG9jU9mjg/PIo24uo3mK3CbAL8Eonda9A9a74LWzfBpxme3PbG1G9DOhDZbvzgZ+Wet8PPE713oiDACQNLeW/7aqDjmhPEoGIqJv9qd5nT/m9P9VJ/gzb8wFsP0P1rPrHbU8rZc+3rO/AAuCShs87Srq7PPZ2J2BDSUOANWxfVup91fbLtqcA75W0Wonpkibai1hseddARNSGpFWoTsgbSTLVc+sNzKDt11239Qz2+bz9ImrZhuVXbS8obS0LnA6Ms/0XSRPLth29L/k8YALVlMXBTR5WxGLJiEBE1Mk+wLm21ypvLVwTeASYCRwqaUkASSsDDwDvlrR5KRtS1v8JGCNpCUlrAuPbaaslQXiq3JewD1QjC8BjkvYs9S5T3hgIMAk4qmx3b5cddUQHkghERJ3sD1zWquwS4N1Ur7GdI2k28HHbrwP7AqeWsuupTu63UyUPc4HvUyUR/8T2P4D/LttdTvUq3RafAI6QNAe4A3hX2efvVK8P/sViHmdE0/L2wYiIPqKMDMwFNrP9XG/HE/WQEYGIiD5A0i5U0xGnJgmInpQRgYiIiBrLiEBERESNJRGIiIiosSQCERERNZZEICIiosaSCERERNTY/wfHleet4f8h1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='Accuracy', y='Model', data=output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
