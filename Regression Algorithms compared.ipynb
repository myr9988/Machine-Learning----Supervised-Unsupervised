{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73a4896",
   "metadata": {},
   "source": [
    "**This notebook compares the performance of common regression algorithms for the same dataset.\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Random Forest\n",
    "\n",
    "XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1572b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHREE\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3434cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data and selecting the independent and independent variables \n",
    "\n",
    "train_data = pd.read_csv(r'C:\\Users\\SHREE\\Downloads\\Python CODES\\Predicting house prices\\train.csv')\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "y = train_data.SalePrice\n",
    "X = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ae5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data for training and testing and cleaning it using imputation \n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e3a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using Decision Tress : 28042.449315068494\n"
     ]
    }
   ],
   "source": [
    "# making predictions using the Decision Tree algorithm \n",
    "\n",
    "decision_model = DecisionTreeRegressor()  \n",
    "\n",
    "decision_model.fit(train_X, train_y) \n",
    "\n",
    "predicted_decision_trees = decision_model.predict(test_X)\n",
    "\n",
    "print (\"Mean Absolute Error using Decision Tress :\", mean_absolute_error(test_y, predicted_decision_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0539214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using Random Forest: 18668.63364643969\n"
     ]
    }
   ],
   "source": [
    "# making predictions using the Random Forest algorithm \n",
    "\n",
    "forest_model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "forest_model.fit(train_X, train_y )\n",
    "\n",
    "predicted_random_forest = forest_model.predict(test_X)\n",
    "\n",
    "print(\"Mean Absolute Error using Random Forest:\", mean_absolute_error(test_y, predicted_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be915e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using XGBoost:  19082.452846746575\n"
     ]
    }
   ],
   "source": [
    "# making predictions using the XGBoost algorithm \n",
    "\n",
    "xg_model = XGBRegressor(n_estimators=100)\n",
    "\n",
    "xg_model.fit(train_X, train_y)\n",
    "\n",
    "predicted_XGBoost = xg_model.predict(test_X)\n",
    "\n",
    "print(\"Mean Absolute Error using XGBoost: \", mean_absolute_error(test_y, predicted_XGBoost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b7031",
   "metadata": {},
   "source": [
    "As evident from the above metrics, we can conclude that using a combination of Decision Trees (Random Forest) can prove to be very useful in bringing down the errors (increaing accuracy). Also, further improvement in the results can be made using some kind of boosting algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
